{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":2876308,"sourceType":"datasetVersion","datasetId":1761664},{"sourceId":8776641,"sourceType":"datasetVersion","datasetId":5275114},{"sourceId":8776656,"sourceType":"datasetVersion","datasetId":5275124}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T08:01:17.164679Z","iopub.execute_input":"2024-07-11T08:01:17.165041Z","iopub.status.idle":"2024-07-11T08:01:18.135623Z","shell.execute_reply.started":"2024-07-11T08:01:17.165013Z","shell.execute_reply":"2024-07-11T08:01:18.134609Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/list-of-bad-words/full-list-of-bad-words_text-file_2022_05_05.txt\n/kaggle/input/google-profanity/googleprofanity(1000).txt\n/kaggle/input/profanities-in-english-collection/profanity_en.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers torch sklearn\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:18.137526Z","iopub.execute_input":"2024-07-11T08:01:18.137893Z","iopub.status.idle":"2024-07-11T08:01:20.918964Z","shell.execute_reply.started":"2024-07-11T08:01:18.137867Z","shell.execute_reply":"2024-07-11T08:01:20.917816Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:45:48.190263Z","iopub.execute_input":"2024-07-11T14:45:48.191054Z","iopub.status.idle":"2024-07-11T14:45:49.634569Z","shell.execute_reply.started":"2024-07-11T14:45:48.191020Z","shell.execute_reply":"2024-07-11T14:45:49.633425Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n...                  ...                                                ...   \n159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n0           0             0        0       0       0              0  \n1           0             0        0       0       0              0  \n2           0             0        0       0       0              0  \n3           0             0        0       0       0              0  \n4           0             0        0       0       0              0  \n...       ...           ...      ...     ...     ...            ...  \n159566      0             0        0       0       0              0  \n159567      0             0        0       0       0              0  \n159568      0             0        0       0       0              0  \n159569      0             0        0       0       0              0  \n159570      0             0        0       0       0              0  \n\n[159571 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>ffe987279560d7ff</td>\n      <td>\":::::And for the second time of asking, when ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ffea4adeee384e90</td>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>ffee36eab5c267c9</td>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>fff125370e4aaaf3</td>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>fff46fc426af1f9a</td>\n      <td>\"\\nAnd ... I really don't think you understand...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df['__label__1']=df.loc[:,'toxic':'identity_hate'].sum(axis=1).gt(0).astype(int)\n# df['__label__1']","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:22.600545Z","iopub.execute_input":"2024-07-11T08:01:22.601183Z","iopub.status.idle":"2024-07-11T08:01:22.604695Z","shell.execute_reply.started":"2024-07-11T08:01:22.601154Z","shell.execute_reply":"2024-07-11T08:01:22.603798Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# df['__label__2']=(df.loc[:,'toxic':'identity_hate'].sum(axis=1)==0).astype(int)\n# df['__label__2']","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:22.605845Z","iopub.execute_input":"2024-07-11T08:01:22.606177Z","iopub.status.idle":"2024-07-11T08:01:22.613122Z","shell.execute_reply.started":"2024-07-11T08:01:22.606146Z","shell.execute_reply":"2024-07-11T08:01:22.612197Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df['label']=df.loc[:,'toxic':'identity_hate'].sum(axis=1).gt(0).astype(int)\ndf['label']\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:45:53.699826Z","iopub.execute_input":"2024-07-11T14:45:53.700673Z","iopub.status.idle":"2024-07-11T14:45:53.746875Z","shell.execute_reply.started":"2024-07-11T14:45:53.700638Z","shell.execute_reply":"2024-07-11T14:45:53.745786Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"0         0\n1         0\n2         0\n3         0\n4         0\n         ..\n159566    0\n159567    0\n159568    0\n159569    0\n159570    0\nName: label, Length: 159571, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df=df.drop(['toxic', 'severe_toxic','obscene','threat', 'insult', 'identity_hate'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:45:59.864312Z","iopub.execute_input":"2024-07-11T14:45:59.865231Z","iopub.status.idle":"2024-07-11T14:45:59.879621Z","shell.execute_reply.started":"2024-07-11T14:45:59.865194Z","shell.execute_reply":"2024-07-11T14:45:59.878672Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"add_data= pd.read_csv('/kaggle/input/google-profanity/googleprofanity(1000).txt', sep='\\t',header=None)\nadd_data.columns = ['comment_text']\nadd_data['label']=1\nadd_data","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:47:56.026500Z","iopub.execute_input":"2024-07-11T14:47:56.027582Z","iopub.status.idle":"2024-07-11T14:47:56.046269Z","shell.execute_reply.started":"2024-07-11T14:47:56.027544Z","shell.execute_reply":"2024-07-11T14:47:56.045224Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"       comment_text  label\n0     2 girls 1 cup      1\n1              2g1c      1\n2              4r5e      1\n3              5h1t      1\n4              5hit      1\n..              ...    ...\n953            yaoi      1\n954  yellow showers      1\n955           yiffy      1\n956       zoophilia      1\n957               🖕      1\n\n[958 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2 girls 1 cup</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2g1c</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4r5e</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5h1t</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5hit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>yaoi</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>954</th>\n      <td>yellow showers</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>955</th>\n      <td>yiffy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>956</th>\n      <td>zoophilia</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>🖕</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>958 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=pd.concat([df,add_data],axis=0)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:48:17.780756Z","iopub.execute_input":"2024-07-11T14:48:17.781511Z","iopub.status.idle":"2024-07-11T14:48:17.805403Z","shell.execute_reply.started":"2024-07-11T14:48:17.781475Z","shell.execute_reply":"2024-07-11T14:48:17.804453Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"                   id                                       comment_text  \\\n0    0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1    000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2    000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3    0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n4    0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n..                ...                                                ...   \n953               NaN                                               yaoi   \n954               NaN                                     yellow showers   \n955               NaN                                              yiffy   \n956               NaN                                          zoophilia   \n957               NaN                                                  🖕   \n\n     label  \n0        0  \n1        0  \n2        0  \n3        0  \n4        0  \n..     ...  \n953      1  \n954      1  \n955      1  \n956      1  \n957      1  \n\n[160529 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>NaN</td>\n      <td>yaoi</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>954</th>\n      <td>NaN</td>\n      <td>yellow showers</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>955</th>\n      <td>NaN</td>\n      <td>yiffy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>956</th>\n      <td>NaN</td>\n      <td>zoophilia</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>NaN</td>\n      <td>🖕</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>160529 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\n# Compile regex patterns\npattern_punctuation = re.compile(r'[^\\w\\s]')\npattern_whitespace = re.compile(r'\\s+')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:48:22.504466Z","iopub.execute_input":"2024-07-11T14:48:22.504847Z","iopub.status.idle":"2024-07-11T14:48:22.509963Z","shell.execute_reply.started":"2024-07-11T14:48:22.504816Z","shell.execute_reply":"2024-07-11T14:48:22.508877Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Vectorized operations\ndf['comment_text'] = df['comment_text'].str.replace(pattern_punctuation, '', regex=True)  # Remove punctuation\ndf['comment_text'] = df['comment_text'].str.replace(pattern_whitespace, ' ', regex=True)  # Replace multiple whitespace with single space\ndf['comment_text'] = df['comment_text'].str.strip()  # Remove leading and trailing whitespace\ndf['comment_text']=df['comment_text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:48:36.904176Z","iopub.execute_input":"2024-07-11T14:48:36.904932Z","iopub.status.idle":"2024-07-11T14:48:44.058416Z","shell.execute_reply.started":"2024-07-11T14:48:36.904897Z","shell.execute_reply":"2024-07-11T14:48:44.057375Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:48:47.703819Z","iopub.execute_input":"2024-07-11T14:48:47.704700Z","iopub.status.idle":"2024-07-11T14:48:47.716899Z","shell.execute_reply.started":"2024-07-11T14:48:47.704664Z","shell.execute_reply":"2024-07-11T14:48:47.715710Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"                   id                                       comment_text  \\\n0    0000997932d777bf  explanation why the edits made under my userna...   \n1    000103f0d9cfb60f  daww he matches this background colour im seem...   \n2    000113f07ec002fd  hey man im really not trying to edit war its j...   \n3    0001b41b1c6bb37e  more i cant make any real suggestions on impro...   \n4    0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n..                ...                                                ...   \n953               NaN                                               yaoi   \n954               NaN                                     yellow showers   \n955               NaN                                              yiffy   \n956               NaN                                          zoophilia   \n957               NaN                                                      \n\n     label  \n0        0  \n1        0  \n2        0  \n3        0  \n4        0  \n..     ...  \n953      1  \n954      1  \n955      1  \n956      1  \n957      1  \n\n[160529 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>explanation why the edits made under my userna...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>daww he matches this background colour im seem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>hey man im really not trying to edit war its j...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>more i cant make any real suggestions on impro...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>NaN</td>\n      <td>yaoi</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>954</th>\n      <td>NaN</td>\n      <td>yellow showers</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>955</th>\n      <td>NaN</td>\n      <td>yiffy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>956</th>\n      <td>NaN</td>\n      <td>zoophilia</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>NaN</td>\n      <td></td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>160529 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"import spacy\n# Load spaCy English language model\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# Define preprocessing function using spaCy\ndef preprocess(text):\n    doc = nlp(text)\n    # Lemmatize and remove stop words\n    lemmatized_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n    return lemmatized_text\n\n# Apply the preprocessing function\ndf['comment_text'] = df['comment_text'].apply(preprocess)\n","metadata":{}},{"cell_type":"code","source":"\n#import spacy\n# Load spaCy English language model\n#nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# Define preprocessing function using spaCy\n#def preprocess(text):\n#    doc = nlp(text)\n    # Lemmatize and remove stop words\n#    lemmatized_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n#    return lemmatized_text\n\n# Apply the preprocessing function\n#df['processed_text'] = df['comment_text'].apply(preprocess)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:30.782857Z","iopub.execute_input":"2024-07-11T08:01:30.783615Z","iopub.status.idle":"2024-07-11T08:01:30.787545Z","shell.execute_reply.started":"2024-07-11T08:01:30.783584Z","shell.execute_reply":"2024-07-11T08:01:30.786538Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:49:01.630518Z","iopub.execute_input":"2024-07-11T14:49:01.631318Z","iopub.status.idle":"2024-07-11T14:49:01.643482Z","shell.execute_reply.started":"2024-07-11T14:49:01.631284Z","shell.execute_reply":"2024-07-11T14:49:01.642349Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"                   id                                       comment_text  \\\n0    0000997932d777bf  explanation why the edits made under my userna...   \n1    000103f0d9cfb60f  daww he matches this background colour im seem...   \n2    000113f07ec002fd  hey man im really not trying to edit war its j...   \n3    0001b41b1c6bb37e  more i cant make any real suggestions on impro...   \n4    0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n..                ...                                                ...   \n953               NaN                                               yaoi   \n954               NaN                                     yellow showers   \n955               NaN                                              yiffy   \n956               NaN                                          zoophilia   \n957               NaN                                                      \n\n     label  \n0        0  \n1        0  \n2        0  \n3        0  \n4        0  \n..     ...  \n953      1  \n954      1  \n955      1  \n956      1  \n957      1  \n\n[160529 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>explanation why the edits made under my userna...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>daww he matches this background colour im seem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>hey man im really not trying to edit war its j...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>more i cant make any real suggestions on impro...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>NaN</td>\n      <td>yaoi</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>954</th>\n      <td>NaN</td>\n      <td>yellow showers</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>955</th>\n      <td>NaN</td>\n      <td>yiffy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>956</th>\n      <td>NaN</td>\n      <td>zoophilia</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>NaN</td>\n      <td></td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>160529 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# XLNet","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n# from transformers import XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:30.856274Z","iopub.execute_input":"2024-07-11T08:01:30.856573Z","iopub.status.idle":"2024-07-11T08:01:47.614126Z","shell.execute_reply.started":"2024-07-11T08:01:30.856543Z","shell.execute_reply":"2024-07-11T08:01:47.613345Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"2024-07-11 08:01:38.825044: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-11 08:01:38.825166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-11 08:01:38.930559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n# from nltk.stem import WordNetLemmatizer\n# from nltk.corpus import wordnet\n# from nltk import pos_tag\n# import nltk\n\n# # Download necessary NLTK data\n# nltk.download('wordnet')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('omw-1.4')\n\n\n\n# # Function to get the part of speech tag for lemmatization\n# def get_wordnet_pos(word):\n#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n#     tag = pos_tag([word])[0][1][0].upper()\n#     tag_dict = {\"J\": wordnet.ADJ,\n#                 \"N\": wordnet.NOUN,\n#                 \"V\": wordnet.VERB,\n#                 \"R\": wordnet.ADV}\n#     return tag_dict.get(tag, wordnet.NOUN)\n\n# # Initialize lemmatizer\n# lemmatizer = WordNetLemmatizer()\n\n# # Function to preprocess text\n# def preprocess(text):\n#     # Remove punctuation\n#     text = re.sub(r'[^\\w\\s]', '', text)\n#     # Remove multiple whitespace\n#     text = re.sub(r'\\s+', ' ', text).strip()\n#     # Tokenize and remove stopwords\n#     tokens = [word for word in text.split() if word.lower() not in ENGLISH_STOP_WORDS]\n#      #Lemmatize tokens\n#     lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n#     return ' '.join(lemmatized_tokens)\n\n# # Apply preprocessing function\n# df['clean_text'] = df['comment_text'].apply(preprocess)\n\n# print(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:47.629382Z","iopub.execute_input":"2024-07-11T08:01:47.629735Z","iopub.status.idle":"2024-07-11T08:01:47.660289Z","shell.execute_reply.started":"2024-07-11T08:01:47.629702Z","shell.execute_reply":"2024-07-11T08:01:47.659495Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:47.661510Z","iopub.execute_input":"2024-07-11T08:01:47.661844Z","iopub.status.idle":"2024-07-11T08:02:00.230825Z","shell.execute_reply.started":"2024-07-11T08:01:47.661814Z","shell.execute_reply":"2024-07-11T08:02:00.229831Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Is Actually Helpful","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport re\n\nnltk.data.path.append(\"/kaggle/working/\")\n# Ensure necessary NLTK data is downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('corpora')\nimport nltk\n\nnltk.download('wordnet')\n\n\n# Now you can use WordNet functionalities\nfrom nltk.corpus import wordnet\nimport nltk\nnltk.download('wordnet')\n\nfrom nltk.corpus import wordnet\n\n# Load data\ndata = df.copy()\n\n# Initialize lemmatizer\nlemmatizer = WordNetLemmatizer()\n# Tokenize text\ndata['tokens'] = data['comment_text'].apply(word_tokenize)\n\n# Remove stop words\nstop_words = set(stopwords.words('english'))\ndata['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n\n# Lemmatize tokens\ndata['tokens'] = data['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n\n# Join tokens back to a single string\ndata['processed_text'] = data['tokens'].apply(lambda x: ' '.join(x))\n\n# TF-IDF features\ntfidf = TfidfVectorizer(max_features=5000)\ntfidf_features = tfidf.fit_transform(data['processed_text']).toarray()\ntfidf_df = pd.DataFrame(tfidf_features, columns=tfidf.get_feature_names_out())\ndata = pd.concat([data, tfidf_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:49:28.829536Z","iopub.execute_input":"2024-07-11T14:49:28.829923Z","iopub.status.idle":"2024-07-11T14:51:09.728035Z","shell.execute_reply.started":"2024-07-11T14:49:28.829892Z","shell.execute_reply":"2024-07-11T14:51:09.726456Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Error loading corpora: Package 'corpora' not found in\n[nltk_data]     index\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/kaggle/working/'\n    - '/kaggle/working/'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[85], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Lemmatize tokens\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Join tokens back to a single string\u001b[39;00m\n\u001b[1;32m     45\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[85], line 42\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     39\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Lemmatize tokens\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Join tokens back to a single string\u001b[39;00m\n\u001b[1;32m     45\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n","Cell \u001b[0;32mIn[85], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Lemmatize tokens\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Join tokens back to a single string\u001b[39;00m\n\u001b[1;32m     45\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/kaggle/working/'\n    - '/kaggle/working/'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/kaggle/working/'\n    - '/kaggle/working/'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:51:21.859850Z","iopub.execute_input":"2024-07-11T14:51:21.860976Z","iopub.status.idle":"2024-07-11T14:51:21.877911Z","shell.execute_reply.started":"2024-07-11T14:51:21.860942Z","shell.execute_reply":"2024-07-11T14:51:21.876693Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"                   id                                       comment_text  \\\n0    0000997932d777bf  explanation why the edits made under my userna...   \n1    000103f0d9cfb60f  daww he matches this background colour im seem...   \n2    000113f07ec002fd  hey man im really not trying to edit war its j...   \n3    0001b41b1c6bb37e  more i cant make any real suggestions on impro...   \n4    0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n..                ...                                                ...   \n953               NaN                                               yaoi   \n954               NaN                                     yellow showers   \n955               NaN                                              yiffy   \n956               NaN                                          zoophilia   \n957               NaN                                                      \n\n     label                                             tokens  \n0        0  [explanation, edits, made, username, hardcore,...  \n1        0  [daww, matches, background, colour, im, seemin...  \n2        0  [hey, man, im, really, trying, edit, war, guy,...  \n3        0  [cant, make, real, suggestions, improvement, w...  \n4        0         [sir, hero, chance, remember, page, thats]  \n..     ...                                                ...  \n953      1                                             [yaoi]  \n954      1                                  [yellow, showers]  \n955      1                                            [yiffy]  \n956      1                                        [zoophilia]  \n957      1                                                 []  \n\n[160529 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>label</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>explanation why the edits made under my userna...</td>\n      <td>0</td>\n      <td>[explanation, edits, made, username, hardcore,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>daww he matches this background colour im seem...</td>\n      <td>0</td>\n      <td>[daww, matches, background, colour, im, seemin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>hey man im really not trying to edit war its j...</td>\n      <td>0</td>\n      <td>[hey, man, im, really, trying, edit, war, guy,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>more i cant make any real suggestions on impro...</td>\n      <td>0</td>\n      <td>[cant, make, real, suggestions, improvement, w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>you sir are my hero any chance you remember wh...</td>\n      <td>0</td>\n      <td>[sir, hero, chance, remember, page, thats]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>NaN</td>\n      <td>yaoi</td>\n      <td>1</td>\n      <td>[yaoi]</td>\n    </tr>\n    <tr>\n      <th>954</th>\n      <td>NaN</td>\n      <td>yellow showers</td>\n      <td>1</td>\n      <td>[yellow, showers]</td>\n    </tr>\n    <tr>\n      <th>955</th>\n      <td>NaN</td>\n      <td>yiffy</td>\n      <td>1</td>\n      <td>[yiffy]</td>\n    </tr>\n    <tr>\n      <th>956</th>\n      <td>NaN</td>\n      <td>zoophilia</td>\n      <td>1</td>\n      <td>[zoophilia]</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>NaN</td>\n      <td></td>\n      <td>1</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>160529 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data[['comment_text','label','tokens']].to_csv(\"cleaned_dataset\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:52:19.794484Z","iopub.execute_input":"2024-07-11T14:52:19.794897Z","iopub.status.idle":"2024-07-11T14:52:25.742500Z","shell.execute_reply.started":"2024-07-11T14:52:19.794868Z","shell.execute_reply":"2024-07-11T14:52:25.741709Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"data.label","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:53:05.279082Z","iopub.execute_input":"2024-07-11T14:53:05.279702Z","iopub.status.idle":"2024-07-11T14:53:05.287269Z","shell.execute_reply.started":"2024-07-11T14:53:05.279668Z","shell.execute_reply":"2024-07-11T14:53:05.286334Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n953    1\n954    1\n955    1\n956    1\n957    1\nName: label, Length: 160529, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":" texts = data['comment_text'].tolist()  \n labels = df['label'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:53:07.469385Z","iopub.execute_input":"2024-07-11T14:53:07.469807Z","iopub.status.idle":"2024-07-11T14:53:07.486911Z","shell.execute_reply.started":"2024-07-11T14:53:07.469775Z","shell.execute_reply":"2024-07-11T14:53:07.485807Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:53:09.714560Z","iopub.execute_input":"2024-07-11T14:53:09.715359Z","iopub.status.idle":"2024-07-11T14:53:09.824361Z","shell.execute_reply.started":"2024-07-11T14:53:09.715327Z","shell.execute_reply":"2024-07-11T14:53:09.823541Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n# import torch\n# # Load tokenizer and model\n# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(labels)))\n\n# # Tokenize data\n# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# # Convert to PyTorch datasets\n# class Dataset(torch.utils.data.Dataset):\n#     def __init__(self, encodings, labels):\n#         self.encodings = encodings\n#         self.labels = labels\n\n#     def __getitem__(self, idx):\n#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n#         item['labels'] = torch.tensor(self.labels[idx])\n#         return item\n\n#     def __len__(self):\n#         return len(self.labels)\n\n# train_dataset = Dataset(train_encodings, train_labels)\n# val_dataset = Dataset(val_encodings, val_labels)\n\n# # # Define training arguments\n# # training_args = TrainingArguments(\n# #     output_dir='./results',\n# #     num_train_epochs=3,\n# #     per_device_train_batch_size=8,\n# #     per_device_eval_batch_size=8,\n# #     warmup_steps=500,\n# #     weight_decay=0.01,\n# #     logging_dir='./logs',\n# #     logging_steps=10,\n# #     evaluation_strategy=\"epoch\"\n# # )\n\n# training_args = TrainingArguments(\n#     output_dir='/kaggle/working/',\n#     num_train_epochs=3,\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n#     warmup_steps=500,\n#     weight_decay=0.01,\n#     logging_dir='/kaggle/working/',\n#     logging_steps=10,\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",  # Save checkpoints only at the end of each epoch\n#     save_total_limit=1,      # Keep only the last checkpoint\n#     report_to=\"none\"         # Disable wandb\n# )\n\n# # Define metrics function\n# def compute_metrics(pred):   \n#     labels = pred.label_ids\n#     preds = pred.predictions.argmax(-1)\n#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n#     acc = accuracy_score(labels, preds)\n#     return {\n#         'accuracy': acc,\n#         'f1': f1,\n#         'precision': precision,\n#         'recall': recall\n#     }\n\n# # Create Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     compute_metrics=compute_metrics\n# )\n\n# # # Train the model\n# # trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:06:09.886465Z","iopub.execute_input":"2024-07-11T08:06:09.887188Z","iopub.status.idle":"2024-07-11T08:12:53.069082Z","shell.execute_reply.started":"2024-07-11T08:06:09.887154Z","shell.execute_reply":"2024-07-11T08:12:53.068277Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# training_args = TrainingArguments(\n#     output_dir='/kaggle/working/',\n#     num_train_epochs=3,\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n#     warmup_steps=500,\n#     weight_decay=0.01,\n#     logging_dir='/kaggle/working/',\n#     logging_steps=10,\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",  # Save checkpoints only at the end of each epoch\n#     save_total_limit=1,      # Keep only the last checkpoint\n#     report_to=\"none\"         # Disable wandb\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:39.668743Z","iopub.status.idle":"2024-07-11T08:03:39.669045Z","shell.execute_reply.started":"2024-07-11T08:03:39.668893Z","shell.execute_reply":"2024-07-11T08:03:39.668906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (Training on GPU)","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED'] = 'true'\n\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\n# Load tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(labels)))\n\n# Tokenize data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Convert to PyTorch datasets\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Dataset(train_encodings, train_labels)\nval_dataset = Dataset(val_encodings, val_labels)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='/kaggle/working/',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",  # Save checkpoints only at the end of each epoch\n    save_total_limit=1,     # Keep only the last checkpoint\n    report_to=\"none\"        # Disable wandb\n)\n\n# Define metrics function\ndef compute_metrics(pred):   \n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# # Train the model\n# trainer.train()\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:14:44.452908Z","iopub.execute_input":"2024-07-11T08:14:44.453284Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24070' max='47871' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24070/47871 1:37:44 < 1:36:39, 4.10 it/s, Epoch 1.51/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.017900</td>\n      <td>0.133622</td>\n      <td>0.964562</td>\n      <td>0.963996</td>\n      <td>0.963656</td>\n      <td>0.964562</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED'] = 'true'\n\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\n# Load tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(labels)))\n\n# Tokenize data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Convert to PyTorch datasets\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Dataset(train_encodings, train_labels)\nval_dataset = Dataset(val_encodings, val_labels)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='/kaggle/working/',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",  # Save checkpoints only at the end of each epoch\n    save_total_limit=1,     # Keep only the last checkpoint\n    report_to=\"none\"        # Disable wandb\n)\n\n# Define metrics function\ndef compute_metrics(pred):   \n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# # Train the model\n# trainer.train()\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:53:57.700882Z","iopub.execute_input":"2024-07-11T14:53:57.701245Z","iopub.status.idle":"2024-07-11T18:21:43.953238Z","shell.execute_reply.started":"2024-07-11T14:53:57.701218Z","shell.execute_reply":"2024-07-11T18:21:43.952109Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='48159' max='48159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [48159/48159 3:20:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.136400</td>\n      <td>0.135708</td>\n      <td>0.961565</td>\n      <td>0.961272</td>\n      <td>0.961041</td>\n      <td>0.961565</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.007100</td>\n      <td>0.162036</td>\n      <td>0.964212</td>\n      <td>0.962590</td>\n      <td>0.963077</td>\n      <td>0.964212</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.033100</td>\n      <td>0.151891</td>\n      <td>0.964991</td>\n      <td>0.964370</td>\n      <td>0.964085</td>\n      <td>0.964991</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=48159, training_loss=0.11059915130697588, metrics={'train_runtime': 12046.9328, 'train_samples_per_second': 31.981, 'train_steps_per_second': 3.998, 'total_flos': 5.103558221279846e+16, 'train_loss': 0.11059915130697588, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model\nmodel_path = \"/kaggle/working/\"\ntrainer.save_model(model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:23:17.293655Z","iopub.execute_input":"2024-07-11T18:23:17.294015Z","iopub.status.idle":"2024-07-11T18:23:17.906687Z","shell.execute_reply.started":"2024-07-11T18:23:17.293988Z","shell.execute_reply":"2024-07-11T18:23:17.905899Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Create a zip file of the model directory\nshutil.make_archive('/kaggle/working/model_checkpoint_d', 'zip', '/kaggle/working/checkpoint-48159')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:24:15.843461Z","iopub.execute_input":"2024-07-11T18:24:15.844169Z","iopub.status.idle":"2024-07-11T18:24:57.514068Z","shell.execute_reply.started":"2024-07-11T18:24:15.844136Z","shell.execute_reply":"2024-07-11T18:24:57.512907Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model_checkpoint_d.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch\n\n# Load tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Load model using config.json and model.safetensors\nmodel_path = \"/kaggle/working/checkpoint-48159\"  # Directory containing config.json and model.safetensors\nmodel = DistilBertForSequenceClassification.from_pretrained(model_path)\n\n# Function to predict the class of a given text\ndef predict(text):\n    # Tokenize input text\n    inputs = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n\n    # Forward pass through the model\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Get predicted class\n    logits = outputs.logits\n    predicted_class = logits.argmax().item()\n\n    return predicted_class\n\n# Example text\ntext = \"dick\"\n\n# Make prediction\npredicted_class = predict(text)\nprint(f\"Predicted class: {predicted_class}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:25:34.644136Z","iopub.execute_input":"2024-07-11T18:25:34.644548Z","iopub.status.idle":"2024-07-11T18:25:35.097535Z","shell.execute_reply.started":"2024-07-11T18:25:34.644516Z","shell.execute_reply":"2024-07-11T18:25:35.096582Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"Predicted class: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"dumb\"\n\n# Make prediction\npredicted_class = predict(text)\nprint(f\"Predicted class: {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:25:37.932590Z","iopub.execute_input":"2024-07-11T18:25:37.933260Z","iopub.status.idle":"2024-07-11T18:25:37.962371Z","shell.execute_reply.started":"2024-07-11T18:25:37.933226Z","shell.execute_reply":"2024-07-11T18:25:37.961359Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Predicted class: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['ass', 'fucker', 'dum', 'butt', 'sick', 'love', 'cum', 'creampie']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\"Predicted class: {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:25:45.351809Z","iopub.execute_input":"2024-07-11T18:25:45.352763Z","iopub.status.idle":"2024-07-11T18:25:45.527653Z","shell.execute_reply.started":"2024-07-11T18:25:45.352726Z","shell.execute_reply":"2024-07-11T18:25:45.526651Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Predicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 0\nPredicted class: 1\nPredicted class: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['sucker', 'dirt', 'pus', 'penis', 'nude']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\"Predicted class: {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:25:50.260698Z","iopub.execute_input":"2024-07-11T18:25:50.261071Z","iopub.status.idle":"2024-07-11T18:25:50.369038Z","shell.execute_reply.started":"2024-07-11T18:25:50.261041Z","shell.execute_reply":"2024-07-11T18:25:50.368112Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Predicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['orgasm', 'cunt', 'cameltoe', 'lesbian', 'gay', 'virgin', 'pornstar','slut']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\"Predicted class: {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:25:55.077716Z","iopub.execute_input":"2024-07-11T18:25:55.078347Z","iopub.status.idle":"2024-07-11T18:25:55.253064Z","shell.execute_reply.started":"2024-07-11T18:25:55.078312Z","shell.execute_reply":"2024-07-11T18:25:55.252024Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Predicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\nPredicted class: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['this is damn hot girl', 'i like the way u kiss me', 'Your skin is soft as silk', 'Your shirt is hot as fuck', 'motherfucker get away from my way', 'motherfvcker', 'i dont give a shit to pornhub','You are so cool']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\" {text} is {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:26:26.058887Z","iopub.execute_input":"2024-07-11T18:26:26.059253Z","iopub.status.idle":"2024-07-11T18:26:26.308938Z","shell.execute_reply.started":"2024-07-11T18:26:26.059224Z","shell.execute_reply":"2024-07-11T18:26:26.307924Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":" this is damn hot girl is 1\n i like the way u kiss me is 0\n Your skin is soft as silk is 0\n Your shirt is hot as fuck is 1\n motherfucker get away from my way is 1\n motherfvcker is 1\n i dont give a shit to pornhub is 1\n You are so cool is 0\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['gonna ride on this bitch', 'crazy people', 'you nigga', 'dicklicker', 'your puss', 'motherfvcker', 'you  bustard ugly fatso','boob']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\" {text} is {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:27:22.770682Z","iopub.execute_input":"2024-07-11T18:27:22.771053Z","iopub.status.idle":"2024-07-11T18:27:23.001523Z","shell.execute_reply.started":"2024-07-11T18:27:22.771024Z","shell.execute_reply":"2024-07-11T18:27:23.000527Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":" gonna ride on this bitch is 1\n crazy people is 1\n you nigga is 1\n dicklicker is 1\n your puss is 1\n motherfvcker is 1\n you  bustard ugly fatso is 1\n boob is 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['anal', 'gangbang', 'balllicker', 'bangbros', 'barenaked', 'bigtits', 'bi-sexual','cock', 'brestjob', 'bullshit', 'clit','clitorius','dildo','doggie-style','freak','jizz','nigro','piss','seduce', 'semen','masterbate','murder','panties','pee','undress', 'upskirt']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\" {text} is {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:27:37.154110Z","iopub.execute_input":"2024-07-11T18:27:37.154763Z","iopub.status.idle":"2024-07-11T18:27:37.734823Z","shell.execute_reply.started":"2024-07-11T18:27:37.154729Z","shell.execute_reply":"2024-07-11T18:27:37.733740Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":" anal is 1\n gangbang is 1\n balllicker is 1\n bangbros is 1\n barenaked is 1\n bigtits is 1\n bi-sexual is 1\n cock is 1\n brestjob is 1\n bullshit is 1\n clit is 1\n clitorius is 1\n dildo is 1\n doggie-style is 1\n freak is 1\n jizz is 1\n nigro is 1\n piss is 1\n seduce is 1\n semen is 1\n masterbate is 1\n murder is 1\n panties is 1\n pee is 1\n undress is 1\n upskirt is 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['gonna ride on this b!tch', 'you bastard', 'you ugly little fatso ', 'murder', 'gonna have fun with hot babes', 'f4ck', 'seduce','Homo Sapiens']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\" {text} is {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:30:20.108781Z","iopub.execute_input":"2024-07-11T18:30:20.109151Z","iopub.status.idle":"2024-07-11T18:30:20.332740Z","shell.execute_reply.started":"2024-07-11T18:30:20.109122Z","shell.execute_reply":"2024-07-11T18:30:20.331759Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":" gonna ride on this b!tch is 0\n you bastard is 1\n you ugly little fatso  is 1\n murder is 1\n gonna have fun with hot babes is 0\n f4ck is 1\n seduce is 1\n Homo Sapiens is 1\n","output_type":"stream"}]},{"cell_type":"code","source":"test=['drug', 'virgin', 'feeling lusty', 'My libido is very high', 'butthole', 'shemale', 'i ride a bicycle','married couple having fun']\nfor text in test:\n    predicted_class = predict(text)\n    print(f\" {text} is {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:32:42.519519Z","iopub.execute_input":"2024-07-11T18:32:42.520485Z","iopub.status.idle":"2024-07-11T18:32:42.737846Z","shell.execute_reply.started":"2024-07-11T18:32:42.520438Z","shell.execute_reply":"2024-07-11T18:32:42.736747Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":" drug is 0\n virgin is 1\n feeling lusty is 0\n My libido is very high is 0\n butthole is 1\n shemale is 1\n i ride a bicycle is 0\n married couple having fun is 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# more testing ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Replace 'path_to_testdata.txt' with the actual path to your test data file\ntestdata_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n\ntestdata_df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:06:11.377665Z","iopub.execute_input":"2024-07-11T14:06:11.378057Z","iopub.status.idle":"2024-07-11T14:06:12.909949Z","shell.execute_reply.started":"2024-07-11T14:06:11.378029Z","shell.execute_reply":"2024-07-11T14:06:12.908946Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"                      id                                       comment_text\n0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3       00017563c3f7919a  :If you have a look back at the source, the in...\n4       00017695ad8997eb          I don't anonymously edit articles at all.\n...                  ...                                                ...\n153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n\n[153164 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>:If you have a look back at the source, the in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>I don't anonymously edit articles at all.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>153159</th>\n      <td>fffcd0960ee309b5</td>\n      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n    </tr>\n    <tr>\n      <th>153160</th>\n      <td>fffd7a9a6eb32c16</td>\n      <td>== Throw from out field to home plate. == \\n\\n...</td>\n    </tr>\n    <tr>\n      <th>153161</th>\n      <td>fffda9e8d6fafa9e</td>\n      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n    </tr>\n    <tr>\n      <th>153162</th>\n      <td>fffe8f1340a79fc2</td>\n      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n    </tr>\n    <tr>\n      <th>153163</th>\n      <td>ffffce3fb183ee80</td>\n      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n    </tr>\n  </tbody>\n</table>\n<p>153164 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Vectorized operations\ntestdata_df['comment_text'] = testdata_df['comment_text'].str.replace(pattern_punctuation, '', regex=True)  # Remove punctuation\ntestdata_df['comment_text'] = testdata_df['comment_text'].str.replace(pattern_whitespace, ' ', regex=True)  # Replace multiple whitespace with single space\ntestdata_df['comment_text'] = testdata_df['comment_text'].str.strip()  # Remove leading and trailing whitespace\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:12:58.335171Z","iopub.execute_input":"2024-07-11T14:12:58.335896Z","iopub.status.idle":"2024-07-11T14:13:05.775850Z","shell.execute_reply.started":"2024-07-11T14:12:58.335859Z","shell.execute_reply":"2024-07-11T14:13:05.774921Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\ntest_labels","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:16:34.734620Z","iopub.execute_input":"2024-07-11T14:16:34.735033Z","iopub.status.idle":"2024-07-11T14:16:34.966139Z","shell.execute_reply.started":"2024-07-11T14:16:34.735002Z","shell.execute_reply":"2024-07-11T14:16:34.965115Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"                      id  toxic  severe_toxic  obscene  threat  insult  \\\n0       00001cee341fdb12     -1            -1       -1      -1      -1   \n1       0000247867823ef7     -1            -1       -1      -1      -1   \n2       00013b17ad220c46     -1            -1       -1      -1      -1   \n3       00017563c3f7919a     -1            -1       -1      -1      -1   \n4       00017695ad8997eb     -1            -1       -1      -1      -1   \n...                  ...    ...           ...      ...     ...     ...   \n153159  fffcd0960ee309b5     -1            -1       -1      -1      -1   \n153160  fffd7a9a6eb32c16     -1            -1       -1      -1      -1   \n153161  fffda9e8d6fafa9e     -1            -1       -1      -1      -1   \n153162  fffe8f1340a79fc2     -1            -1       -1      -1      -1   \n153163  ffffce3fb183ee80     -1            -1       -1      -1      -1   \n\n        identity_hate  \n0                  -1  \n1                  -1  \n2                  -1  \n3                  -1  \n4                  -1  \n...               ...  \n153159             -1  \n153160             -1  \n153161             -1  \n153162             -1  \n153163             -1  \n\n[153164 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>153159</th>\n      <td>fffcd0960ee309b5</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>153160</th>\n      <td>fffd7a9a6eb32c16</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>153161</th>\n      <td>fffda9e8d6fafa9e</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>153162</th>\n      <td>fffe8f1340a79fc2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>153163</th>\n      <td>ffffce3fb183ee80</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>153164 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"> **test_labels has some vulnerability**","metadata":{}},{"cell_type":"markdown","source":"# Train on TPU","metadata":{}},{"cell_type":"code","source":"# import os\n# import tensorflow as tf\n# from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, create_optimizer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n# import pandas as pd\n\n# # Disable W&B\n# os.environ['WANDB_DISABLED'] = 'true'\n\n# # Load data\n\n# texts = data['comment_text'].tolist()\n# labels = df['label'].tolist()\n\n# # Split data\n# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n\n# # Load tokenizer and model\n# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# # Convert to TensorFlow datasets\n# def create_tf_dataset(encodings, labels):\n#     def gen():\n#         for i in range(len(labels)):\n#             yield {key: tf.convert_to_tensor(val[i], dtype=tf.int32) for key, val in encodings.items()}, tf.convert_to_tensor(labels[i], dtype=tf.int32)\n#     return tf.data.Dataset.from_generator(gen, ({key: tf.int32 for key in encodings.keys()}, tf.int32))\n\n# train_dataset = create_tf_dataset(train_encodings, train_labels)\n# val_dataset = create_tf_dataset(val_encodings, val_labels)\n\n# # Detect TPU\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     raise BaseException('ERROR: Not connected to a TPU runtime; please check your settings.')\n\n# # Connect to the TPU cluster\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # Create TPU strategy\n# strategy = tf.distribute.TPUStrategy(tpu)\n\n# # Define metrics function\n# def compute_metrics(pred):   \n#     labels = pred.label_ids\n#     preds = tf.argmax(pred.predictions, axis=-1)\n#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n#     acc = accuracy_score(labels, preds)\n#     return {\n#         'accuracy': acc,\n#         'f1': f1,\n#         'precision': precision,\n#         'recall': recall\n#     }\n\n# # Train the model within TPU strategy scope\n# with strategy.scope():\n#     model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(labels)))\n    \n#     # Define optimizer\n#     steps_per_epoch = len(train_dataset) // 8\n#     num_train_steps = steps_per_epoch * 3\n#     optimizer, _ = create_optimizer(init_lr=3e-5, num_train_steps=num_train_steps, num_warmup_steps=500)\n\n#     model.compile(optimizer=optimizer, \n#                   loss=model.compute_loss, \n#                   metrics=['accuracy'])\n    \n#     model.fit(train_dataset.batch(8), epochs=3, validation_data=val_dataset.batch(8))\n\n# # Evaluate the model\n# results = model.evaluate(val_dataset.batch(8))\n# print(\"Validation loss:\", results[0])\n# print(\"Validation accuracy:\", results[1])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:39.678146Z","iopub.status.idle":"2024-07-11T08:03:39.678478Z","shell.execute_reply.started":"2024-07-11T08:03:39.678298Z","shell.execute_reply":"2024-07-11T08:03:39.678311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n\n# # Detect TPU\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     raise BaseException('ERROR: Not connected to a TPU runtime; please check your settings.')\n\n# # Connect to the TPU cluster\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # Create TPU strategy\n# strategy = tf.distribute.TPUStrategy(tpu)\n\n# with strategy.scope():\n#     # Your model and training code goes here\n#     trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:39.679426Z","iopub.status.idle":"2024-07-11T08:03:39.679786Z","shell.execute_reply.started":"2024-07-11T08:03:39.679622Z","shell.execute_reply":"2024-07-11T08:03:39.679637Z"},"trusted":true},"execution_count":null,"outputs":[]}]}